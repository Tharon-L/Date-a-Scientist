{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (59946, 31)\n",
      "Missing values per column:\n",
      "age              0\n",
      "body_type     5296\n",
      "diet         24395\n",
      "drinks        2985\n",
      "drugs        14080\n",
      "education     6628\n",
      "essay0        5488\n",
      "essay1        7572\n",
      "essay2        9638\n",
      "essay3       11476\n",
      "dtype: int64\n",
      "Creating advanced features...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from sklearn.impute import SimpleImputer\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "class DatingProfileAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.attractiveness_model = None\n",
    "        self.compatibility_model = None\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def load_data(self, filepath='profiles.csv'):\n",
    "        \"\"\"Load and perform initial data exploration\"\"\"\n",
    "        self.df = pd.read_csv(filepath)\n",
    "        print(f\"Dataset shape: {self.df.shape}\")\n",
    "        print(f\"Missing values per column:\\n{self.df.isnull().sum().head(10)}\")\n",
    "        return self.df\n",
    "    \n",
    "    def create_advanced_features(self):\n",
    "        \"\"\"Create comprehensive feature engineering\"\"\"\n",
    "        print(\"Creating advanced features...\")\n",
    "        \n",
    "        # Basic profile completeness\n",
    "        self.df['completeness'] = self.df.notna().mean(axis=1)\n",
    "        \n",
    "        # Essay features\n",
    "        essay_cols = [col for col in self.df.columns if 'essay' in col]\n",
    "        self.df['total_essays'] = self.df[essay_cols].notna().sum(axis=1)\n",
    "        \n",
    "        # Combined essay text\n",
    "        self.df['all_essays'] = self.df[essay_cols].fillna('').apply(\n",
    "            lambda x: ' '.join(x.astype(str)), axis=1\n",
    "        )\n",
    "        \n",
    "        # Text length and complexity features\n",
    "        self.df['essay_length'] = self.df['all_essays'].apply(len)\n",
    "        self.df['word_count'] = self.df['all_essays'].apply(lambda x: len(x.split()))\n",
    "        self.df['sentence_count'] = self.df['all_essays'].apply(\n",
    "            lambda x: len(re.split(r'[.!?]+', x)) if x else 0\n",
    "        )\n",
    "        self.df['avg_word_length'] = self.df['all_essays'].apply(\n",
    "            lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0\n",
    "        )\n",
    "        \n",
    "        # Reading complexity (with error handling)\n",
    "        def safe_flesch_reading_ease(text):\n",
    "            try:\n",
    "                if text and len(text.strip()) > 10:\n",
    "                    return flesch_reading_ease(text)\n",
    "                else:\n",
    "                    return 50  # neutral score\n",
    "            except:\n",
    "                return 50\n",
    "        \n",
    "        self.df['reading_ease'] = self.df['all_essays'].apply(safe_flesch_reading_ease)\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        sentiment_scores = self.df['all_essays'].apply(\n",
    "            lambda x: sid.polarity_scores(x) if x else {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 0}\n",
    "        )\n",
    "        self.df['sentiment_compound'] = [score['compound'] for score in sentiment_scores]\n",
    "        self.df['sentiment_positive'] = [score['pos'] for score in sentiment_scores]\n",
    "        self.df['sentiment_negative'] = [score['neg'] for score in sentiment_scores]\n",
    "        \n",
    "        # Personality indicators from text\n",
    "        self.df['exclamation_count'] = self.df['all_essays'].apply(lambda x: x.count('!'))\n",
    "        self.df['question_count'] = self.df['all_essays'].apply(lambda x: x.count('?'))\n",
    "        self.df['emoji_count'] = self.df['all_essays'].apply(\n",
    "            lambda x: len(re.findall(r'[ðŸ˜€-ðŸ™]', x)) if x else 0\n",
    "        )\n",
    "        \n",
    "        # Lifestyle consistency features\n",
    "        lifestyle_cols = ['drinks', 'smokes', 'drugs']\n",
    "        for col in lifestyle_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[f'{col}_filled'] = self.df[col].notna().astype(int)\n",
    "        \n",
    "        # Age-related features\n",
    "        if 'age' in self.df.columns:\n",
    "            self.df['age_group'] = pd.cut(self.df['age'], \n",
    "                                        bins=[0, 25, 30, 35, 40, 100], \n",
    "                                        labels=['18-25', '26-30', '31-35', '36-40', '40+'])\n",
    "        \n",
    "        # Photo-related features (if available)\n",
    "        if 'photos' in self.df.columns:\n",
    "            self.df['has_photos'] = self.df['photos'].notna().astype(int)\n",
    "        \n",
    "        print(f\"Created {len([col for col in self.df.columns if col not in self.original_columns])} new features\")\n",
    "    \n",
    "    def create_attractiveness_target(self, method='composite'):\n",
    "        \"\"\"Create sophisticated attractiveness target variable\"\"\"\n",
    "        if method == 'composite':\n",
    "            # Weighted composite score\n",
    "            weights = {\n",
    "                'completeness': 0.25,\n",
    "                'essay_quality': 0.35,\n",
    "                'engagement': 0.20,\n",
    "                'lifestyle': 0.20\n",
    "            }\n",
    "            \n",
    "            # Normalize essay length (longer essays show effort)\n",
    "            essay_score = np.clip(self.df['essay_length'] / self.df['essay_length'].quantile(0.9), 0, 1)\n",
    "            \n",
    "            # Engagement score (sentiment + interactivity)\n",
    "            engagement_score = (\n",
    "                (self.df['sentiment_compound'] + 1) / 2 * 0.6 +  # Normalize sentiment to 0-1\n",
    "                np.clip((self.df['exclamation_count'] + self.df['question_count']) / 10, 0, 1) * 0.4\n",
    "            )\n",
    "            \n",
    "            # Lifestyle score (completeness in lifestyle fields)\n",
    "            lifestyle_cols = ['drinks', 'smokes', 'drugs', 'diet']\n",
    "            lifestyle_score = self.df[[col for col in lifestyle_cols if col in self.df.columns]].notna().mean(axis=1)\n",
    "            \n",
    "            # Composite attractiveness score\n",
    "            self.df['attractiveness_score'] = (\n",
    "                weights['completeness'] * self.df['completeness'] +\n",
    "                weights['essay_quality'] * essay_score +\n",
    "                weights['engagement'] * engagement_score +\n",
    "                weights['lifestyle'] * lifestyle_score\n",
    "            )\n",
    "            \n",
    "        # Binary target for classification\n",
    "        self.df['high_attractiveness'] = (\n",
    "            self.df['attractiveness_score'] > self.df['attractiveness_score'].quantile(0.7)\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\"Attractiveness distribution: {self.df['high_attractiveness'].value_counts().to_dict()}\")\n",
    "    \n",
    "    def create_compatibility_pairs(self, n_pairs=2000):\n",
    "        \"\"\"Create more sophisticated compatibility pairs with reduced memory usage\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        print(f\"Creating {n_pairs} user pairs...\")\n",
    "        \n",
    "        # Sample a subset of users for pair creation to manage memory\n",
    "        n_users = min(10000, len(self.df))  # Limit to 10k users max\n",
    "        sampled_users = self.df.sample(n=n_users, random_state=42).index\n",
    "        \n",
    "        # Create stratified pairs (some positive, some negative examples)\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        \n",
    "        # Group users by key characteristics for positive pairs\n",
    "        for orientation in self.df.loc[sampled_users, 'orientation'].dropna().unique()[:5]:  # Limit orientations\n",
    "            subset = self.df[(self.df.index.isin(sampled_users)) & (self.df['orientation'] == orientation)]\n",
    "            if len(subset) > 1:\n",
    "                # Create positive pairs within same orientation\n",
    "                n_pos_pairs = min(50, len(subset)//2)  # Reduced number\n",
    "                for _ in range(n_pos_pairs):\n",
    "                    if len(positive_pairs) >= n_pairs // 3:  # Limit positive pairs\n",
    "                        break\n",
    "                    pair = np.random.choice(subset.index, 2, replace=False)\n",
    "                    positive_pairs.append({'user1': pair[0], 'user2': pair[1], 'label': 1})\n",
    "        \n",
    "        # Create negative pairs (random pairs)\n",
    "        remaining_pairs = n_pairs - len(positive_pairs)\n",
    "        for _ in range(remaining_pairs):\n",
    "            pair = np.random.choice(sampled_users, 2, replace=False)\n",
    "            negative_pairs.append({'user1': pair[0], 'user2': pair[1], 'label': 0})\n",
    "        \n",
    "        # Combine pairs\n",
    "        all_pairs = positive_pairs + negative_pairs\n",
    "        self.user_pairs = pd.DataFrame(all_pairs)\n",
    "        \n",
    "        # Add pair features\n",
    "        self._add_pair_features()\n",
    "        \n",
    "        print(f\"Created {len(self.user_pairs)} user pairs\")\n",
    "        print(f\"Compatibility distribution: {self.user_pairs['compatible'].value_counts().to_dict()}\")\n",
    "    \n",
    "    def _add_pair_features(self):\n",
    "        \"\"\"Add features for user pairs\"\"\"\n",
    "        # Merge user data\n",
    "        user1_data = self.df.add_suffix('_1')\n",
    "        user2_data = self.df.add_suffix('_2')\n",
    "        \n",
    "        self.user_pairs = self.user_pairs.merge(\n",
    "            user1_data[['age_1', 'orientation_1', 'diet_1', 'religion_1', 'attractiveness_score_1']], \n",
    "            left_on='user1', right_index=True, how='left'\n",
    "        )\n",
    "        self.user_pairs = self.user_pairs.merge(\n",
    "            user2_data[['age_2', 'orientation_2', 'diet_2', 'religion_2', 'attractiveness_score_2']], \n",
    "            left_on='user2', right_index=True, how='left'\n",
    "        )\n",
    "        \n",
    "        # Compatibility features\n",
    "        self.user_pairs['age_diff'] = abs(self.user_pairs['age_1'] - self.user_pairs['age_2'])\n",
    "        self.user_pairs['age_compatible'] = (self.user_pairs['age_diff'] <= 10).astype(int)\n",
    "        \n",
    "        # Shared characteristics\n",
    "        for col in ['orientation', 'diet', 'religion']:\n",
    "            if f'{col}_1' in self.user_pairs.columns:\n",
    "                self.user_pairs[f'same_{col}'] = (\n",
    "                    self.user_pairs[f'{col}_1'] == self.user_pairs[f'{col}_2']\n",
    "                ).astype(int)\n",
    "        \n",
    "        # Attractiveness compatibility\n",
    "        self.user_pairs['attractiveness_diff'] = abs(\n",
    "            self.user_pairs['attractiveness_score_1'] - self.user_pairs['attractiveness_score_2']\n",
    "        )\n",
    "        self.user_pairs['attractiveness_match'] = (self.user_pairs['attractiveness_diff'] <= 0.2).astype(int)\n",
    "        \n",
    "        # Create target based on multiple factors\n",
    "        self.user_pairs['compatible'] = (\n",
    "            (self.user_pairs.get('same_orientation', 0) == 1) & \n",
    "            (self.user_pairs['age_compatible'] == 1) &\n",
    "            (self.user_pairs['attractiveness_match'] == 1)\n",
    "        ).astype(int)\n",
    "    \n",
    "    def build_attractiveness_model(self):\n",
    "        \"\"\"Build enhanced attractiveness prediction model\"\"\"\n",
    "        print(\"Building attractiveness prediction model...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        text_features = ['all_essays']\n",
    "        categorical_features = ['age_group', 'body_type', 'diet', 'drinks', 'orientation']\n",
    "        numerical_features = [\n",
    "            'completeness', 'essay_length', 'word_count', 'reading_ease',\n",
    "            'sentiment_compound', 'sentiment_positive', 'exclamation_count'\n",
    "        ]\n",
    "        \n",
    "        # Filter available features\n",
    "        categorical_features = [f for f in categorical_features if f in self.df.columns]\n",
    "        numerical_features = [f for f in numerical_features if f in self.df.columns]\n",
    "        \n",
    "        # Prepare data\n",
    "        feature_cols = text_features + categorical_features + numerical_features\n",
    "        X = self.df[feature_cols].copy()\n",
    "        y = self.df['high_attractiveness'].copy()\n",
    "        \n",
    "        # Handle missing values in text\n",
    "        X['all_essays'] = X['all_essays'].fillna('')\n",
    "        \n",
    "        # Preprocessing pipeline with reduced memory usage\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('text', TfidfVectorizer(max_features=500, stop_words='english', ngram_range=(1,2)), 'all_essays'),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), categorical_features),\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_features)\n",
    "        ])\n",
    "        \n",
    "        # Model pipeline with hyperparameter tuning\n",
    "        self.attractiveness_model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ])\n",
    "        \n",
    "        # Simplified hyperparameter tuning for large datasets\n",
    "        param_grid = {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [15, None],\n",
    "            'classifier__min_samples_split': [5, 10]\n",
    "        }\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        # Grid search with memory optimization\n",
    "        grid_search = GridSearchCV(\n",
    "            self.attractiveness_model, \n",
    "            param_grid, \n",
    "            cv=3, \n",
    "            scoring='roc_auc', \n",
    "            n_jobs=1,  # Avoid memory issues with parallel processing\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        self.attractiveness_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate\n",
    "        train_preds = self.attractiveness_model.predict(X_train)\n",
    "        test_preds = self.attractiveness_model.predict(X_test)\n",
    "        test_probs = self.attractiveness_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Train Accuracy: {accuracy_score(y_train, train_preds):.3f}\")\n",
    "        print(f\"Test Accuracy: {accuracy_score(y_test, test_preds):.3f}\")\n",
    "        print(f\"Test ROC-AUC: {roc_auc_score(y_test, test_probs):.3f}\")\n",
    "        print(f\"Test F1: {f1_score(y_test, test_preds):.3f}\")\n",
    "        \n",
    "        return X_test, y_test, test_preds, test_probs\n",
    "    \n",
    "    def build_compatibility_model(self):\n",
    "        \"\"\"Build enhanced compatibility matching model\"\"\"\n",
    "        print(\"\\nBuilding compatibility prediction model...\")\n",
    "        \n",
    "        # Feature selection\n",
    "        compatibility_features = [\n",
    "            'age_diff', 'age_compatible', 'attractiveness_diff', 'attractiveness_match'\n",
    "        ]\n",
    "        \n",
    "        # Add categorical matching features if available\n",
    "        for col in ['orientation', 'diet', 'religion']:\n",
    "            if f'same_{col}' in self.user_pairs.columns:\n",
    "                compatibility_features.append(f'same_{col}')\n",
    "        \n",
    "        # Prepare data\n",
    "        X = self.user_pairs[compatibility_features].copy()\n",
    "        y = self.user_pairs['compatible'].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Try multiple models\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "            avg_score = cv_scores.mean()\n",
    "            \n",
    "            print(f\"{name} CV F1: {avg_score:.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_model = model\n",
    "        \n",
    "        # Train best model\n",
    "        self.compatibility_model = best_model\n",
    "        self.compatibility_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_preds = self.compatibility_model.predict(X_train)\n",
    "        test_preds = self.compatibility_model.predict(X_test)\n",
    "        \n",
    "        print(f\"\\nBest model: {type(best_model).__name__}\")\n",
    "        print(f\"Train F1: {f1_score(y_train, train_preds):.3f}\")\n",
    "        print(f\"Test F1: {f1_score(y_test, test_preds):.3f}\")\n",
    "        print(f\"Test Accuracy: {accuracy_score(y_test, test_preds):.3f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(self.compatibility_model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': compatibility_features,\n",
    "                'importance': self.compatibility_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(f\"\\nTop feature importances:\")\n",
    "            print(importance_df.head())\n",
    "        \n",
    "        return X_test, y_test, test_preds\n",
    "    \n",
    "    def predict_attractiveness(self, profile_data):\n",
    "        \"\"\"Predict attractiveness for new profile\"\"\"\n",
    "        if self.attractiveness_model is None:\n",
    "            raise ValueError(\"Attractiveness model not trained yet!\")\n",
    "        \n",
    "        # Convert to DataFrame if needed\n",
    "        if isinstance(profile_data, dict):\n",
    "            profile_data = pd.DataFrame([profile_data])\n",
    "        \n",
    "        prediction = self.attractiveness_model.predict(profile_data)[0]\n",
    "        probability = self.attractiveness_model.predict_proba(profile_data)[0]\n",
    "        \n",
    "        return {\n",
    "            'prediction': bool(prediction),\n",
    "            'probability_high': probability[1],\n",
    "            'confidence': max(probability)\n",
    "        }\n",
    "    \n",
    "    def predict_compatibility(self, user1_id, user2_id):\n",
    "        \"\"\"Predict compatibility between two users\"\"\"\n",
    "        if self.compatibility_model is None:\n",
    "            raise ValueError(\"Compatibility model not trained yet!\")\n",
    "        \n",
    "        # Create pair features\n",
    "        pair_data = self._create_pair_features(user1_id, user2_id)\n",
    "        \n",
    "        prediction = self.compatibility_model.predict([pair_data])[0]\n",
    "        if hasattr(self.compatibility_model, 'predict_proba'):\n",
    "            probability = self.compatibility_model.predict_proba([pair_data])[0]\n",
    "        else:\n",
    "            probability = [1-prediction, prediction]\n",
    "        \n",
    "        return {\n",
    "            'prediction': bool(prediction),\n",
    "            'probability_compatible': probability[1] if len(probability) > 1 else prediction,\n",
    "            'confidence': max(probability) if len(probability) > 1 else abs(prediction - 0.5) + 0.5\n",
    "        }\n",
    "    \n",
    "    def _create_pair_features(self, user1_id, user2_id):\n",
    "        \"\"\"Create features for a specific user pair\"\"\"\n",
    "        user1 = self.df.loc[user1_id]\n",
    "        user2 = self.df.loc[user2_id]\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Age difference\n",
    "        features.append(abs(user1.get('age', 30) - user2.get('age', 30)))\n",
    "        features.append(int(features[0] <= 10))  # age_compatible\n",
    "        \n",
    "        # Attractiveness difference\n",
    "        attr_diff = abs(user1.get('attractiveness_score', 0.5) - user2.get('attractiveness_score', 0.5))\n",
    "        features.append(attr_diff)\n",
    "        features.append(int(attr_diff <= 0.2))  # attractiveness_match\n",
    "        \n",
    "        # Categorical matches\n",
    "        for col in ['orientation', 'diet', 'religion']:\n",
    "            if col in user1 and col in user2:\n",
    "                features.append(int(user1[col] == user2[col]))\n",
    "            else:\n",
    "                features.append(0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def generate_insights(self):\n",
    "        \"\"\"Generate insights from the trained models\"\"\"\n",
    "        insights = {\n",
    "            'dataset_summary': {\n",
    "                'total_profiles': len(self.df),\n",
    "                'high_attractiveness_rate': self.df['high_attractiveness'].mean(),\n",
    "                'avg_essay_length': self.df['essay_length'].mean(),\n",
    "                'avg_completeness': self.df['completeness'].mean()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if hasattr(self, 'user_pairs'):\n",
    "            insights['compatibility_summary'] = {\n",
    "                'total_pairs': len(self.user_pairs),\n",
    "                'compatibility_rate': self.user_pairs['compatible'].mean(),\n",
    "                'avg_age_difference': self.user_pairs['age_diff'].mean()\n",
    "            }\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def visualize_results(self, X_test_attr=None, y_test_attr=None, test_probs_attr=None,\n",
    "                         X_test_comp=None, y_test_comp=None, test_preds_comp=None):\n",
    "        \"\"\"Create visualizations of model performance\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Attractiveness model visualizations\n",
    "        if test_probs_attr is not None:\n",
    "            # ROC Curve\n",
    "            from sklearn.metrics import roc_curve\n",
    "            fpr, tpr, _ = roc_curve(y_test_attr, test_probs_attr)\n",
    "            axes[0, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test_attr, test_probs_attr):.3f})')\n",
    "            axes[0, 0].plot([0, 1], [0, 1], 'k--')\n",
    "            axes[0, 0].set_xlabel('False Positive Rate')\n",
    "            axes[0, 0].set_ylabel('True Positive Rate')\n",
    "            axes[0, 0].set_title('Attractiveness Model - ROC Curve')\n",
    "            axes[0, 0].legend()\n",
    "            \n",
    "            # Feature importance (if available)\n",
    "            if hasattr(self.attractiveness_model.named_steps['classifier'], 'feature_importances_'):\n",
    "                # This is simplified - in practice you'd need to map back to original features\n",
    "                axes[0, 1].bar(range(10), sorted(self.attractiveness_model.named_steps['classifier'].feature_importances_)[-10:])\n",
    "                axes[0, 1].set_title('Top 10 Feature Importances - Attractiveness')\n",
    "                axes[0, 1].set_xlabel('Features')\n",
    "                axes[0, 1].set_ylabel('Importance')\n",
    "        \n",
    "        # Data distribution visualizations\n",
    "        axes[0, 2].hist(self.df['attractiveness_score'], bins=30, alpha=0.7)\n",
    "        axes[0, 2].set_title('Attractiveness Score Distribution')\n",
    "        axes[0, 2].set_xlabel('Attractiveness Score')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        \n",
    "        # Compatibility model visualizations\n",
    "        if test_preds_comp is not None:\n",
    "            # Confusion matrix\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            cm = confusion_matrix(y_test_comp, test_preds_comp)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 0])\n",
    "            axes[1, 0].set_title('Compatibility Model - Confusion Matrix')\n",
    "            axes[1, 0].set_xlabel('Predicted')\n",
    "            axes[1, 0].set_ylabel('Actual')\n",
    "        \n",
    "        # Age difference vs compatibility\n",
    "        if hasattr(self, 'user_pairs'):\n",
    "            compatible_pairs = self.user_pairs[self.user_pairs['compatible'] == 1]\n",
    "            incompatible_pairs = self.user_pairs[self.user_pairs['compatible'] == 0]\n",
    "            \n",
    "            axes[1, 1].hist(compatible_pairs['age_diff'], alpha=0.7, label='Compatible', bins=20)\n",
    "            axes[1, 1].hist(incompatible_pairs['age_diff'], alpha=0.7, label='Incompatible', bins=20)\n",
    "            axes[1, 1].set_title('Age Difference Distribution by Compatibility')\n",
    "            axes[1, 1].set_xlabel('Age Difference')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].legend()\n",
    "            \n",
    "            # Compatibility rate by age difference\n",
    "            age_comp = self.user_pairs.groupby(pd.cut(self.user_pairs['age_diff'], bins=10))['compatible'].mean()\n",
    "            age_comp.plot(kind='bar', ax=axes[1, 2], rot=45)\n",
    "            axes[1, 2].set_title('Compatibility Rate by Age Difference')\n",
    "            axes[1, 2].set_xlabel('Age Difference Range')\n",
    "            axes[1, 2].set_ylabel('Compatibility Rate')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize analyzer\n",
    "    analyzer = DatingProfileAnalyzer()\n",
    "    \n",
    "    # Load and process data\n",
    "    df = analyzer.load_data('profiles.csv')  # Make sure you have this file\n",
    "    analyzer.original_columns = list(df.columns)\n",
    "    \n",
    "    # Feature engineering\n",
    "    analyzer.create_advanced_features()\n",
    "    analyzer.create_attractiveness_target()\n",
    "    analyzer.create_compatibility_pairs(n_pairs=2000)  # Reduced for memory efficiency\n",
    "    \n",
    "    # Build models\n",
    "    X_test_attr, y_test_attr, test_preds_attr, test_probs_attr = analyzer.build_attractiveness_model()\n",
    "    X_test_comp, y_test_comp, test_preds_comp = analyzer.build_compatibility_model()\n",
    "    \n",
    "    # Generate insights\n",
    "    insights = analyzer.generate_insights()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INSIGHTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for category, data in insights.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        for key, value in data.items():\n",
    "            print(f\"  {key}: {value:.3f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "    \n",
    "    # Visualizations\n",
    "    analyzer.visualize_results(\n",
    "        X_test_attr, y_test_attr, test_probs_attr,\n",
    "        X_test_comp, y_test_comp, test_preds_comp\n",
    "    )\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Run the enhanced analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (59946, 31)\n",
      "Missing values per column:\n",
      "age              0\n",
      "body_type     5296\n",
      "diet         24395\n",
      "drinks        2985\n",
      "drugs        14080\n",
      "education     6628\n",
      "essay0        5488\n",
      "essay1        7572\n",
      "essay2        9638\n",
      "essay3       11476\n",
      "dtype: int64\n",
      "Creating advanced features...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m DatingProfileAnalyzer()\n\u001b[0;32m      3\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofiles.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mcreate_advanced_features()\n\u001b[0;32m      5\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mcreate_attractiveness_target()\n\u001b[0;32m      6\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mbuild_attractiveness_model()\n",
      "Cell \u001b[1;32mIn[34], line 75\u001b[0m, in \u001b[0;36mDatingProfileAnalyzer.create_advanced_features\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Sentiment analysis\u001b[39;00m\n\u001b[0;32m     74\u001b[0m sid \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[1;32m---> 75\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_essays\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: sid\u001b[38;5;241m.\u001b[39mpolarity_scores(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_compound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m sentiment_scores]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_positive\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m sentiment_scores]\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[34], line 76\u001b[0m, in \u001b[0;36mDatingProfileAnalyzer.create_advanced_features.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Sentiment analysis\u001b[39;00m\n\u001b[0;32m     74\u001b[0m sid \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m     75\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_essays\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: sid\u001b[38;5;241m.\u001b[39mpolarity_scores(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_compound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m sentiment_scores]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_positive\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m sentiment_scores]\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:366\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03mReturn a float for sentiment strength based on the input text.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03mPositive values are positive valence, negative value are negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    matched as if it was a normal word in the sentence.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m sentitext \u001b[38;5;241m=\u001b[39m SentiText(\n\u001b[0;32m    367\u001b[0m     text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mPUNC_LIST, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mREGEX_REMOVE_PUNCTUATION\n\u001b[0;32m    368\u001b[0m )\n\u001b[0;32m    369\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    370\u001b[0m words_and_emoticons \u001b[38;5;241m=\u001b[39m sentitext\u001b[38;5;241m.\u001b[39mwords_and_emoticons\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:274\u001b[0m, in \u001b[0;36mSentiText.__init__\u001b[1;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNC_LIST \u001b[38;5;241m=\u001b[39m punc_list\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREGEX_REMOVE_PUNCTUATION \u001b[38;5;241m=\u001b[39m regex_remove_punctuation\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_and_emoticons \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words_and_emoticons()\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# doesn't separate words from\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# adjacent punctuation (keeps emoticons & contractions)\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cap_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallcap_differential(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_and_emoticons)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:306\u001b[0m, in \u001b[0;36mSentiText._words_and_emoticons\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03mRemoves leading and trailing puncutation\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03mLeaves contractions and most emoticons\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    Does not preserve punc-plus-letter emoticons (e.g. :D)\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m wes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m--> 306\u001b[0m words_punc_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words_plus_punc()\n\u001b[0;32m    307\u001b[0m wes \u001b[38;5;241m=\u001b[39m [we \u001b[38;5;28;01mfor\u001b[39;00m we \u001b[38;5;129;01min\u001b[39;00m wes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(we) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, we \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wes):\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\nltk\\sentiment\\vader.py:296\u001b[0m, in \u001b[0;36mSentiText._words_plus_punc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m punc_after \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(p): p[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m product(words_only, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNC_LIST)}\n\u001b[0;32m    295\u001b[0m words_punc_dict \u001b[38;5;241m=\u001b[39m punc_before\n\u001b[1;32m--> 296\u001b[0m words_punc_dict\u001b[38;5;241m.\u001b[39mupdate(punc_after)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words_punc_dict\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize and run\n",
    "analyzer = DatingProfileAnalyzer()\n",
    "analyzer.load_data('profiles.csv')\n",
    "analyzer.create_advanced_features()\n",
    "analyzer.create_attractiveness_target()\n",
    "analyzer.build_attractiveness_model()\n",
    "\n",
    "# Make predictions\n",
    "result = analyzer.predict_attractiveness(new_profile_data)\n",
    "print(f\"High attractiveness: {result['prediction']} (confidence: {result['confidence']:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
